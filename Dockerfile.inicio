# bigdata-inicio Dockerfile
FROM bigdata-apoyo

ENV HIVE_VERSION=4.0.0
ENV PYTHON_DEPS="catboost findspark ipykernel lightgbm matplotlib nltk numpy openpyxl optuna pandas plotly pycaret pyspark==3.5.0 rapidfuzz scikit-learn scipy seaborn spacy statsmodels tensorflow-cpu xlsxwriter"

USER $USERNAME
WORKDIR /home/$USERNAME

# Install Hive
RUN wget https://archive.apache.org/dist/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz \
    && tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz -C /tmp/ \
    && sudo mv /tmp/apache-hive-${HIVE_VERSION}-bin /opt/hive \
    && rm apache-hive-${HIVE_VERSION}-bin.tar.gz \
    && sudo mkdir -p /opt/hive/warehouse \
    && sudo chown -R $USERNAME:$USERNAME /opt/hive

# Create Python virtual environment and install Jupyter with dependencies
RUN python3 -m venv /home/$USERNAME/venv \
    && . /home/$USERNAME/venv/bin/activate \
	&& pip install --no-cache-dir --upgrade pip wheel setuptools \
    && pip install --no-cache-dir jupyterlab $PYTHON_DEPS \
	&& python3 -m spacy download es_core_news_lg \
    && echo "source /home/$USERNAME/venv/bin/activate" >> /home/$USERNAME/.bashrc

# Configure Hive environment
ENV HIVE_HOME=/opt/hive
ENV PATH=$PATH:$HIVE_HOME/bin

# SoluciÃ³n SLF4J y permisos
RUN rm -f $HIVE_HOMElib/log4j-slf4j-impl-*.jar && \
    cp ${HADOOP_HOME}/share/hadoop/common/lib/slf4j-reload4j-*.jar $HIVE_HOME/lib/

# Generate Hive configuration
RUN mkdir -p /home/$USERNAME/config/hive \
    && cat > /home/$USERNAME/config/hive/hive-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:derby:;databaseName=/tmp/hive/metastore_db;create=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/opt/hive/warehouse</value>
    </property>
</configuration>
EOF

# Generate Spark configuration
RUN mkdir -p /home/$USERNAME/config/spark \
    && cat > /home/$USERNAME/config/spark/spark-defaults.conf <<EOF
spark.master            yarn
spark.driver.memory     1g
spark.yarn.am.memory    1g
spark.executor.memory   1g
spark.executor.cores    1
spark.serializer        org.apache.spark.serializer.KryoSerializer
spark.history.ui.port	18080
spark.history.fs.logDirectory	file:/tmp/spark-events
spark.eventLog.enabled	true
EOF

RUN cat > /home/$USERNAME/config/spark/spark-env.sh <<EOF
#!/bin/bash
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop
export SPARK_DIST_CLASSPATH=\$(hadoop classpath)
EOF

RUN chmod +x /home/$USERNAME/config/spark/spark-env.sh

# Set log directory
RUN mkdir -p /tmp/spark-events

# Copy entrypoint script
COPY --chown=$USERNAME:$USERNAME entrypoint-inicio.sh /home/$USERNAME/entrypoint.sh
RUN chmod +x /home/$USERNAME/entrypoint.sh

EXPOSE 8088 9870 9864 8042 4040 7077 8888 9083 2181 9092

ENTRYPOINT ["/home/bigdata/entrypoint.sh"]
